[
    {
        "title": "Challenges of language technologies for the indigenous languages of the Americas",
        "date": "August 2018",
        "abstract": "Indigenous languages of the American continent are highly diverse. However, they have received little attention from the technological perspective. In this paper, we review the research, the digital resources and the available NLP systems that focus on these languages. We present the main challenges and research questions that arise when distant languages and low-resource scenarios are faced. We would like to encourage NLP research in linguistically rich and diverse areas like the Americas.",
        "link": "https://aclanthology.org/C18-1006.pdf",
        "image": "forest.jpg"
    },
    {
        "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
        "date": "July 2023",
        "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd’s data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.",
        "link": "https://aclanthology.org/2023.findings-acl.868/",
        "image": "indonesia.jpg"
    },
    {
        "title": "AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas",
        "date": "December 2022",
        "abstract": "Little attention has been paid to the development of human language technology for truly low-resource languages—i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.",
        "link": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2022.995667/full",
        "image": "ind1.jpg"
    },
    {
        "title": "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction",
        "date": "June 2023",
        "abstract": "Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.",
        "link": "https://arxiv.org/abs/2306.06804",
        "image": "ind2.jpg"
    },
    {
        "title": "Survey of Low-Resource Machine Translation",
        "date": "September 2022",
        "abstract": "We present a survey covering the state of the art in low-resource machine translation (MT) research. There are currently around 7,000 languages spoken in the world and almost all language pairs lack significant resources for training machine translation models. There has been increasing interest in research addressing the challenge of producing useful translation models when very little translated training data is available. We present a summary of this topical research field and provide a description of the techniques evaluated by researchers in several recent shared tasks in low-resource MT.",
        "link": "https://aclanthology.org/2022.cl-3.6/",
        "image": "ind3.jpg"
    },
    {
        "title": "The Ethical Question – Use of Indigenous Corpora for Large Language Models",
        "date": "May 2024",
        "abstract": "Creating language technology based on language data has become very popular with the recent advances of large language models and neural network technologies. This makes language resources very valuable, and in case of Indigenous languages especially, the scarce resources are even more precious. Given the good results of simply fetching everything you can from the small set of languages dominating the internet and training neural networks, there have been several attempts at doing the same for all languages. However, online Indigenous language resources are not comparable to the ones for dominating languages. They do not represent a broad range of genres and there is no guarantee that targeted forms outnumber erroneous ones. For most languages the people working on the language models do not speak the language of the models and bad performance is thus hard to notice. Problems related to intellectual property rights and copyrights loom high in contemporary discussions on neural language models. For Indigenous languages this issue is more critical than for dominating languages, since the amount of text available is so small (often barely counting millions of words). The use of artificial intelligence (AI) generated text as next-generation training data becomes even more problematic in cases where the available corpora are so small, since the share of AI-generated text will be larger. In this article we address these problems and describe our alternative, an ethical and sustainable way to work with Indigenous languages in the age of large language models.",
        "link": "https://aclanthology.org/2024.lrec-main.1383/",
        "image": "ind4.jpg"
    },
    {
        "title": "Harnessing the Power of Artificial Intelligence to Vitalize Endangered Indigenous Languages: Technologies and Experiences",
        "date": "July 2024",
        "abstract": "Since 2022 we have been exploring application areas and technologies in which Artificial Intelligence (AI) and modern Natural Language Processing (NLP), such as Large Language Models (LLMs), can be employed to foster the usage and facilitate the documentation of Indigenous languages which are in danger of disappearing. We start by discussing the decreasing diversity of languages in the world and how working with Indigenous languages poses unique ethical challenges for AI and NLP. To address those challenges, we propose an alternative development AI cycle based on community engagement and usage. Then, we report encouraging results in the development of high-quality machine learning translators for Indigenous languages by fine-tuning state-of-the-art (SOTA) translators with tiny amounts of data and discuss how to avoid some common pitfalls in the process. We also present prototypes we have built in projects done in 2023 and 2024 with Indigenous communities in Brazil, aimed at facilitating writing, and discuss the development of Indigenous Language Models (ILMs) as a replicable and scalable way to create spell-checkers, next-word predictors, and similar tools. Finally, we discuss how we envision a future for language documentation where dying languages are preserved as interactive language models.",
        "link": "https://arxiv.org/abs/2407.12620",
        "image": "ind5.jpg"
    },
    {
        "title": "Visually Grounded Reasoning across Languages and Cultures",
        "date": "October 2021",
        "abstract": "The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.",
        "link": "https://arxiv.org/abs/2109.13238",
        "image": "ind6.jpg"
    },
    {
        "title": "Grammar Induction for Brazilian Indigenous Languages",
        "date": "Unknown month 2024",
        "abstract": "This paper investigates the issue of grammar induction for Brazilian indigenous languages, mainly focusing on unsupervised methods, but also testing a large language model for the task. Grammar induction poses several challenges, particularly when applied to low-resource languages, a characteristic commonly associated with indigenous languages. The primary objective of this paper is to discover syntactically related words in sentences. In addition to the contributions to linguistic studies, as in language description and structural analysis, grammar induction may help in varied Natural Language Processing tasks, as it could help detect parsing errors, enhance parsing results, and reveal pertinent relations for open information extraction purposes. The findings reveal that, even with a limited corpus, it is feasible to identify syntactically related words, especially for some relations. To the best of our knowledge, this represents a pioneering attempt to undertake grammar induction for Brazilian indigenous languages.",
        "link": "https://aclanthology.org/2024.propor-2.10.pdf",
        "image": "ind7.jpg"
    },
    {
        "title": "When Is Multilinguality A Curse? Language Modeling for 250 High- and Low-Resource Languages",
        "date": "November 2023",
        "abstract": "Multilingual language models are widely used to extend NLP systems to low-resource languages. However, concrete evidence for the effects of multilinguality on language modeling performance in individual languages remains scarce. Here, we pre-train over 10,000 monolingual and multilingual language models for over 250 languages, including multiple language families that are under-studied in NLP. We assess how language modeling performance in each language varies as a function of (1) monolingual dataset size, (2) added multilingual dataset size, (3) linguistic similarity of the added languages, and (4) model size (up to 45M parameters). We find that in moderation, adding multilingual data improves low-resource language modeling performance, similar to increasing low-resource dataset sizes by up to 33%. Improvements depend on the syntactic similarity of the added multilingual data, with marginal additional effects of vocabulary overlap. However, high-resource languages consistently perform worse in multilingual pre-training scenarios. As dataset sizes increase, adding multilingual data begins to hurt performance for both low-resource and high-resource languages, likely due to limited model capacity (the \"curse of multilinguality\"). These results suggest that massively multilingual pre-training may not be optimal for any languages involved, but that more targeted models can significantly improve performance.",
        "link": "https://arxiv.org/abs/2311.09205",
        "image": "ind8.jpg"
    },
    {
        "title": "A New Age Indigenous Instrument: Artificial Intelligence & Its Potential for (De)colonialized Data",
        "date": "Unknown",
        "abstract": "Data governance remains an ongoing concern in Native communities. Almost all aspects of data about Indigenous people—from what is shared and where it is shared—are externally (i.e., non-Native) controlled, which often renders Indigenous peoples invisible from mainstream narratives. This erasure has long resulted in inaccurate and usually stereotypic portrayals of Native Peoples that mischaracterize them, justify their oppression, and ultimately deny them justice. This Article explores whether artificial intelligence (\"AI\") will serve as a \"revolution\" or a \"new colonizer\" for Indigenous peoples—an answer that ultimately hangs on which narratives AI developers embed into their technologies. Without purposefully centering Indigenous Peoples and accurate data from and about them, this emerging technology will only perpetuate colonial narratives and exacerbate existing disparities. Thus, Indigenous data—whether stories, instruments, values, or customs—must guide and serve as the foundation for 21st-century data governance and AI development to promote equity and advance justice for Indigenous Peoples.",
        "link": "https://journals.law.harvard.edu/crcl/wp-content/uploads/sites/80/2023/01/ANewAgeIndigenousInstrument.pdf",
        "image": "tree.jpg"
    },
    {
        "title": "The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali–English and Sinhala–English",
        "date": "November 2019",
        "abstract": "For machine translation, a vast majority of language pairs in the world are considered low-resource because they have little parallel data available. Besides the technical challenges of learning with limited supervision, it is difficult to evaluate methods trained on low-resource language pairs because of the lack of freely and publicly available benchmarks. In this work, we introduce the FLORES evaluation datasets for Nepali–English and Sinhala–English, based on sentences translated from Wikipedia. Compared to English, these are languages with very different morphology and syntax, for which little out-of-domain parallel data is available and for which relatively large amounts of monolingual data are freely available. We describe our process to collect and cross-check the quality of translations, and we report baseline performance using several learning settings: fully supervised, weakly supervised, semi-supervised, and fully unsupervised. Our experiments demonstrate that current state-of-the-art methods perform rather poorly on this benchmark, posing a challenge to the research community working on low-resource MT. Data and code to reproduce our experiments are available at https://github.com/facebookresearch/flores.",
        "link": "https://aclanthology.org/D19-1632/",
        "image": "waterfall.jpg"
    }
]